{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johncoder-30/NLP-translation-model/blob/main/transformer_translation_20k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer based model to translate tamil sentences to english\n",
        "A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the field of natural language processing (NLP) and in computer vision (CV)."
      ],
      "metadata": {
        "id": "U-UrVB2Z6O8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading of Datasets"
      ],
      "metadata": {
        "id": "3KUsQTA07WHf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0SNYcsvpslL",
        "outputId": "db43e5b6-008e-49a6-9fab-ef75e6a28db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-20 03:47:12--  https://storage.googleapis.com/samanantar-public/V0.2/data/en2indic/en-ta.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.201.128, 74.125.69.128, 142.250.152.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.201.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1377236241 (1.3G) [application/zip]\n",
            "Saving to: ‘en-ta.zip’\n",
            "\n",
            "en-ta.zip           100%[===================>]   1.28G   182MB/s    in 6.6s    \n",
            "\n",
            "2022-03-20 03:47:19 (198 MB/s) - ‘en-ta.zip’ saved [1377236241/1377236241]\n",
            "\n",
            "Archive:  /content/en-ta.zip\n",
            "   creating: /content/data/en-ta/\n",
            " extracting: /content/data/en-ta/train.ta  \n",
            " extracting: /content/data/en-ta/train.en  \n",
            "5095764 5095764\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/samanantar-public/V0.2/data/en2indic/en-ta.zip\n",
        "!unzip \"/content/en-ta.zip\" -d \"/content/data/\"\n",
        "english_raw = open('/content/data/en-ta/train.en', 'r',encoding='utf8').read().split('\\n')\n",
        "tamil_raw = open('/content/data/en-ta/train.ta', 'r', encoding='utf8').read().split('\\n')\n",
        "\n",
        "print(len(english_raw), len(tamil_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing libraries"
      ],
      "metadata": {
        "id": "kQudsUny7eGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTp0Ptw1oZ8p"
      },
      "outputs": [],
      "source": [
        "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing Dataset"
      ],
      "metadata": {
        "id": "JX7gsH3J7qb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEeqNBEkoaDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2b7132-9b4b-402a-ae67-86516b06c309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19991, 2)\n"
          ]
        }
      ],
      "source": [
        "# print(english_raw[10],tamil_raw[10])\n",
        "raw_data = {'English': [line for line in english_raw[:20000]],\n",
        "            'Tamil': [line for line in tamil_raw[:20000]]}\n",
        "df = pd.DataFrame(raw_data, columns=['English', 'Tamil'])\n",
        "df=df[df['English'].str.split(' ').map(len) < 100]\n",
        "df=df[df['Tamil'].str.split(' ').map(len) < 100]\n",
        "train, test = train_test_split(df, test_size=0.05,random_state=1234)\n",
        "train.to_csv('train.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)\n",
        "print(df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fwsyVwZoaK-",
        "outputId": "60309672-52ae-4b15-8892-9a8a6e951598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['she', 'says', 'she', 'knows', 'what', 'is', 'going', 'on', 'but', 'can', 'do', 'nothing', 'about', 'it']\n",
            "['என்ன', 'நடக்கிறது', 'என்பது', 'தமக்கு', 'தெரியும்', 'என்றும்', 'ஆனால்', 'தம்மால்', 'எதுவும்', 'செய்யமுடியாது', 'என்றும்', 'கடிதம்', 'எழுதியிருந்தார்']\n"
          ]
        }
      ],
      "source": [
        "def tokenize_eng(sentence):\n",
        "    sentence = re.sub(r'\\n', '', sentence)\n",
        "    # sentence = re.sub(r'[^\\w\\s\\']', '', sentence.lower())\n",
        "    sentence = re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', '', sentence.lower())\n",
        "    return [words for words in sentence.split()]\n",
        "\n",
        "print(tokenize_eng('She says she knows what is going on, but can do nothing about it.'))\n",
        "\n",
        "def tokenize_tam(sentence):\n",
        "    sentence = re.sub(r'\\n', '', sentence)\n",
        "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
        "    sentence = re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', '', sentence)\n",
        "    return [words for words in sentence.split()]\n",
        "\n",
        "print(tokenize_tam('என்ன நடக்கிறது என்பது தமக்கு தெரியும் என்றும் ஆனால், தம்மால் எதுவும் செய்யமுடியாது என்றும் கடிதம் எழுதியிருந்தார்.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using torchtext library to\n",
        ">1. tokenize sentences,\n",
        ">2. build vocabulary \n",
        ">3. splitting into batches to train in GPU"
      ],
      "metadata": {
        "id": "OhExaX8L75Jr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k0tgGfGoaRF"
      },
      "outputs": [],
      "source": [
        "english = Field(init_token='<sos>', eos_token='<eos>', tokenize=tokenize_eng, lower=True, batch_first=False)\n",
        "tamil = Field(init_token='<sos>', eos_token='<eos>', tokenize=tokenize_tam, lower=False, batch_first=False)\n",
        "fields = {'English': ('eng', english), 'Tamil': ('tam', tamil)}\n",
        "train_data, test_data = TabularDataset.splits(path='', train='train.csv', test='test.csv', format='csv', fields=fields)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "tamil.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "train_iterator, test_iterator = BucketIterator.splits((train_data, test_data),\n",
        "                                                      batch_size=128, device='cuda', sort_key=lambda x: len(x.tam),\n",
        "                                                      sort_within_batch=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save Vocabulary"
      ],
      "metadata": {
        "id": "HLEBwYJa7CQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(english,'/content/gdrive/MyDrive/pytorch_models/english_vocab.pth')\n",
        "torch.save(tamil,'/content/gdrive/MyDrive/pytorch_models/tamil_vocab.pth')"
      ],
      "metadata": {
        "id": "VcgwW9da7PCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Vocabulary"
      ],
      "metadata": {
        "id": "YmkQJJIm7Lin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english=torch.load('/content/gdrive/MyDrive/pytorch_models/english_vocab.pth')\n",
        "tamil=torch.load('/content/gdrive/MyDrive/pytorch_models/tamil_vocab.pth')\n",
        "print(english)\n",
        "print(len(english.vocab))\n",
        "print(english.vocab.itos[47])"
      ],
      "metadata": {
        "id": "j3Y564sh-hRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer Model"
      ],
      "metadata": {
        "id": "xaEHpyWI7-I0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rv7jICcoaYL"
      },
      "outputs": [],
      "source": [
        "class Transformer_model(nn.Module):\n",
        "    def __init__(self, embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder_layers,\n",
        "                 num_decoder_layers, feed_forward, dropout_p, max_len, device):\n",
        "        super(Transformer_model, self).__init__()\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "\n",
        "        self.device = device\n",
        "        self.transformer = nn.Transformer(embedding_size, num_heads, num_encoder_layers,num_decoder_layers, feed_forward, dropout_p)\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # src_shape=(src_len,N)\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "        # src_shape=(N,src_len)\n",
        "        return src_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_seq_length, N = src.shape\n",
        "        trg_seq_length, N = trg.shape\n",
        "\n",
        "        src_positions = (torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N).to(self.device))\n",
        "        trg_positions = (torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N).to(self.device))\n",
        "\n",
        "        embed_src = self.dropout(self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
        "        embed_trg = self.dropout(self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(src).to(self.device)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n",
        "\n",
        "        out = self.transformer(embed_src, embed_trg, src_key_padding_mask=src_padding_mask, tgt_mask=trg_mask)\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameters for Model"
      ],
      "metadata": {
        "id": "1htp3Nhk8E2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JSmn91MoafY",
        "outputId": "908929fb-9fb5-43eb-d0b2-be5a1e02d8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10004 9862\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "num_epoch = 100\n",
        "learning_rate = 3e-4\n",
        "batch_size = 128\n",
        "\n",
        "src_vocab_size = len(tamil.vocab)\n",
        "trg_vocab_size = len(english.vocab)\n",
        "print(src_vocab_size,trg_vocab_size)\n",
        "embedding_size = 512\n",
        "num_heads = 4\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.10\n",
        "max_len = 100\n",
        "feed_forward = 2048\n",
        "src_pad_idx = tamil.vocab.stoi['<pad>']\n",
        "\n",
        "model = Transformer_model(embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder_layers, num_decoder_layers, feed_forward,dropout, max_len, device).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "pad_idx = english.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading pre trained Model"
      ],
      "metadata": {
        "id": "046ZJ1YV8LuW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRCw9uq2oa7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32e3710-c14e-40a4-b8a8-7ca3f8aeeeee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do you want to load model:yes\n",
            "Mounted at /content/gdrive\n",
            "english_vocab.pth     seq2seq_transformer_200.pt      tamil_vocab.pth\n",
            "gan_mnist.pt\t      seq2seq_transformer_220.pt      transformer_glove.pt\n",
            "seq2seq_attention.pt  seq2seq_transformer_lakh_53.pt\n"
          ]
        }
      ],
      "source": [
        "# # load model from g_drive\n",
        "# model = Transformer_model(embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder_layers, num_decoder_layers, feed_forward,dropout, max_len, device).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "q=input('do you want to load model:')\n",
        "if q=='yes':\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount('/content/gdrive')\n",
        "    !ls '/content/gdrive/My Drive/pytorch_models'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q=input('Do u want to continue :')\n",
        "if q=='yes':\n",
        "    model_save_name = 'seq2seq_transformer_220.pt'\n",
        "    path = F\"/content/gdrive/My Drive/pytorch_models/{model_save_name}\"\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "\n",
        "    # model.eval()\n",
        "    # - or -\n",
        "    model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rrsDsnQ0cKW",
        "outputId": "da0e74b2-16fe-45fc-de07-d8a0b264c379"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do u want to continue :yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test function to check model output while training"
      ],
      "metadata": {
        "id": "-b3jFT6u8VGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyalqIoPoal3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c53308-bd10-4f7c-f0da-aa3e2b1a8418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos> today we are well along in the lords day <eos> \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_model():\n",
        "    model.eval()\n",
        "    # tam_sen = '<sos> திருவிழாவைக் காண அருகில் இருக்கும் கிராமங் களைச் சேர்ந்தவர்கள் மறவப்பட் டிக்கு படையெடுத்துவந்தனர். <eos>'\n",
        "    # tam_sen = '<sos> கடந்த 5 ஆண்டுகளில் பயனடைந்தோர் மற்றும் செலவின விவரம் பின்வருமாறு <eos>'\n",
        "    # tam_sen = '<sos> சில கலை வரலாற்றாசிரியர்கள் அவர் ஒரு வருடத்திற்கு இரண்டு அல்லது மூன்று ஓவியங்களை மட்டுமே தயாரித்துள்ளதாக தெரிவித்திருக்கிறார்கள். <eos>'\n",
        "    \n",
        "    # tam_sen = '<sos> இது இவரின் இரண்டாவது தமிழ் தொடர் ஆகும். <eos>' #It is the second longest ran Tamil serial.\n",
        "    tam_sen = '<sos> இன்று நாம் கிராமத்திற்கு செல்கிறோம். <eos>'#today we are going to the village\n",
        "    \n",
        "    tam_encoded = []\n",
        "    for x in tokenize_tam(tam_sen):\n",
        "        tam_encoded.append(tamil.vocab.stoi[x])\n",
        "    tam_sen = torch.Tensor(tam_encoded).long().to(device)\n",
        "    tam_sen = tam_sen.reshape(-1, 1)\n",
        "    \n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "    for i in range(100):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(tam_sen, trg_tensor)\n",
        "\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "    for a in translated_sentence:\n",
        "            print(a, end=' ')\n",
        "    print('\\n')\n",
        "test_model()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eng_decoder(sen):\n",
        "    for a in sen:\n",
        "        for b in a:\n",
        "            print(english.vocab.itos[int(b)], end=' ')\n",
        "        print()\n",
        "\n",
        "def save_model():\n",
        "    model_save_name = 'seq2seq_transformer.pt'\n",
        "    path = F\"/content/{model_save_name}\"\n",
        "# torch.save(model.state_dict(), path)\n",
        "    torch.save({\n",
        "    'epoch': _epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss.item(),\n",
        "        }, path)"
      ],
      "metadata": {
        "id": "CqGGsor-JNRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df['Tamil'][122])\n",
        "# print(df['English'][122])199"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT5JD7UlHUfY",
        "outputId": "e16e26a1-d044-4967-c404-9d1d372a7350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "இது இவரின் இரண்டாவது தமிழ் தொடர் ஆகும்.\n",
            "It is the second longest ran Tamil serial.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Transformer Model"
      ],
      "metadata": {
        "id": "8FNP2WsL8hHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHFCJunsoatW"
      },
      "outputs": [],
      "source": [
        "for _epoch in range(epoch,epoch+1):\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        inp_data = batch.tam.to(device)\n",
        "        target = batch.eng.to(device)\n",
        "        # eng_decoder(target)\n",
        "        # print(target.shape,inp_data.shape)\n",
        "\n",
        "        output = model(inp_data, target[:-1,:])\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "        \n",
        "    if _epoch%5==0:\n",
        "        print('epoch:',_epoch,' loss=',loss.item())\n",
        "        test_model()\n",
        "        save_model()\n",
        "        model.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "def translate(sen):\n",
        "    tam_encoded = []\n",
        "    for x in tokenize_tam(sen):\n",
        "        tam_encoded.append(tamil.vocab.stoi[x])\n",
        "    tam_sen = torch.Tensor(tam_encoded).long().to(device)\n",
        "    tam_sen = tam_sen.reshape(-1, 1)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "    for i in range(100):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(tam_sen, trg_tensor)\n",
        "\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    s=''\n",
        "    for a in translated_sentence:\n",
        "        if a =='<unk>':\n",
        "            continue\n",
        "        s+=a+' '\n",
        "    return s\n",
        "\n",
        "\n",
        "\n",
        "tam_sen_train = ['<sos> திருவிழாவைக் காண அருகில் இருக்கும் கிராமங் களைச் சேர்ந்தவர்கள் மறவப்பட் டிக்கு படையெடுத்துவந்தனர். <eos>',\n",
        "                '<sos> கடந்த 5 ஆண்டுகளில் பயனடைந்தோர் மற்றும் செலவின விவரம் பின்வருமாறு <eos>']\n",
        "tam_sen_test = ['<sos> இன்று நாம் கிராமத்திற்கு செல்கிறோம். <eos>','<sos> இது இவரின் இரண்டாவது தமிழ் தொடர் ஆகும். <eos>']\n",
        "\n",
        "eng_sen_train=['<sos> People from nearby villages also came to watch the celebrations <eos> <sos> Details of beneficiaries and expenditure incurred during the last 5 years are <eos> ']\n",
        "eng_sen_test =['<sos> today we are going to the village <eos> <sos> It is the second longest ran Tamil serial <eos>']\n",
        "eng_sen_ai4 = ['<sos> Today we are going to the village <eos> <sos> This is her second Tamil film <eos>']\n",
        "\n",
        "print('-----Translation from training dataset-----')\n",
        "trans_train=''    \n",
        "for z in tam_sen_train:\n",
        "    trans_train+=translate(z)\n",
        "    # trans_train+='#'\n",
        "print(trans_train,'\\n') \n",
        "\n",
        "print('-----Translation from test dataset-----')\n",
        "trans_test=''\n",
        "for z in tam_sen_test:\n",
        "    trans_test+=translate(z)\n",
        "    # trans_test+='#'\n",
        "print(trans_test,'\\n')\n",
        "\n",
        "print('-----Correct translation from Google translate-----')\n",
        "print(eng_sen_test[0],'\\n')\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "print('BLEU score for training data-> {}'.format(sentence_bleu(eng_sen_train, trans_train)))\n",
        "print('BLEU score for testing data-> {}'.format(sentence_bleu(eng_sen_test, trans_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbY8dLU7sVED",
        "outputId": "01e649cb-65ee-43aa-948b-3bbfaf3c478f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Translation from training dataset-----\n",
            "<sos> people from nearby villages also came to watch the celebrations <eos> <sos> details of beneficiaries and expenditure incurred during the last 5 years are <eos>  \n",
            "\n",
            "-----Translation from test dataset-----\n",
            "<sos> today we are well along in the lords day <eos> <sos> it is the second longest film in the lead role <eos>  \n",
            "\n",
            "-----Correct translation from Google translate-----\n",
            "<sos> today we are going to the village <eos> <sos> It is the second longest ran Tamil serial <eos> \n",
            "\n",
            "BLEU score for training data-> 0.9694128141350016\n",
            "BLEU score for testing data-> 0.6143181919006975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save Model"
      ],
      "metadata": {
        "id": "gTiledC58noM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WSYAWzyWEcS"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "model_save_name = 'seq2seq_transformer_260.pt'\n",
        "path = F\"/content/gdrive/My Drive/pytorch_models/{model_save_name}\"\n",
        "# torch.save(model.state_dict(), path)\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss.item(),\n",
        "}, path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transformer_translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1nmTadcRq9RykCGe2yEQz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}